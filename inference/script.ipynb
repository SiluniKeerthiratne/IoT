{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set tensor: Got value of type FLOAT64 but expected type FLOAT32 for input 0, name: images ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m input_image = np.expand_dims(resized_image, axis=\u001b[32m0\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Set the input tensor to the preprocessed image\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43minterpreter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_details\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mindex\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Run the inference\u001b[39;00m\n\u001b[32m     29\u001b[39m interpreter.invoke()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/IoT/Code/myenvIot/lib/python3.11/site-packages/tensorflow/lite/python/interpreter.py:764\u001b[39m, in \u001b[36mInterpreter.set_tensor\u001b[39m\u001b[34m(self, tensor_index, value)\u001b[39m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_tensor\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor_index, value):\n\u001b[32m    749\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Sets the value of the input tensor.\u001b[39;00m\n\u001b[32m    750\u001b[39m \n\u001b[32m    751\u001b[39m \u001b[33;03m  Note this copies data in `value`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    762\u001b[39m \u001b[33;03m    ValueError: If the interpreter could not set the tensor.\u001b[39;00m\n\u001b[32m    763\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interpreter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSetTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: Cannot set tensor: Got value of type FLOAT64 but expected type FLOAT32 for input 0, name: images "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model\n",
    "model_path = \"best-169_float16.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = \"images/2.png\"\n",
    "image = cv2.imread(image_path)\n",
    "height, width, _ = image.shape\n",
    "\n",
    "# Resize the image to match the model's input size (e.g., 640x640)\n",
    "input_size = (1408, 1408)\n",
    "resized_image = cv2.resize(image, input_size)\n",
    "resized_image = resized_image / 255.0  # Normalize to [0, 1]\n",
    "input_image = np.expand_dims(resized_image, axis=0)\n",
    "\n",
    "# Set the input tensor to the preprocessed image\n",
    "interpreter.set_tensor(input_details[0]['index'], input_image)\n",
    "\n",
    "# Run the inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output tensor\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Assuming YOLOv9 output format (adjust based on actual output shape)\n",
    "# For simplicity, let's assume output is a single tensor with shape (1, num_boxes, 6)\n",
    "# where each box is represented as [x, y, w, h, confidence, class_id]\n",
    "\n",
    "# Interpret the output\n",
    "class_ids = []\n",
    "scores = []\n",
    "boxes = []\n",
    "\n",
    "# Assuming output_data is a numpy array\n",
    "for detection in output_data[0]:\n",
    "    scores.append(detection[4])  # Confidence\n",
    "    class_ids.append(int(detection[5]))  # Class ID\n",
    "    # Convert bounding box coordinates to original image scale\n",
    "    x, y, w, h = detection[0:4]\n",
    "    x = int((x - w / 2) * width)\n",
    "    y = int((y - h / 2) * height)\n",
    "    x2 = int((x + w) * width)\n",
    "    y2 = int((y + h) * height)\n",
    "    \n",
    "    # Ensure bounding box coordinates are within image bounds\n",
    "    x = max(0, min(x, width))\n",
    "    y = max(0, min(y, height))\n",
    "    x2 = max(0, min(x2, width))\n",
    "    y2 = max(0, min(y2, height))\n",
    "    \n",
    "    boxes.append([x, y, x2, y2])\n",
    "\n",
    "# Apply Non-Maximum Suppression (NMS) if needed\n",
    "# For simplicity, this step is omitted here. You can use OpenCV's NMSBoxes function.\n",
    "\n",
    "# Draw bounding boxes on the original image\n",
    "for i in range(len(boxes)):\n",
    "    if scores[i] > 0.5:  # Filter by confidence threshold\n",
    "        x, y, x2, y2 = boxes[i]\n",
    "        cv2.rectangle(image, (x, y), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f'Class {class_ids[i]} - {scores[i]:.2f}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image with bounding boxes\n",
    "cv2.imshow('Image', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silunikeerthiratne/Documents/IoT/Code/myenvIot/lib/python3.11/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "[ WARN:0@1.941] global loadsave.cpp:848 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2  # Import OpenCV\n",
    "\n",
    "def load_tflite_model(tflite_file):\n",
    "    \"\"\"Loads a TensorFlow Lite model.\"\"\"\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_file)\n",
    "    interpreter.allocate_tensors()\n",
    "    return interpreter\n",
    "\n",
    "def preprocess_image(image_path, input_size):\n",
    "    \"\"\"\n",
    "    Reads, resizes, and preprocesses an image for TFLite inference.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    img = cv2.resize(img, input_size)\n",
    "    img = img.astype(np.float32) / 255.0  # Normalize\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    return img, cv2.resize(cv2.cvtColor(img[0], cv2.COLOR_RGB2BGR), input_size)  # Return original image for drawing\n",
    "\n",
    "def run_inference(interpreter, image):\n",
    "    \"\"\"Runs inference on a TFLite model.\"\"\"\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], image)\n",
    "    interpreter.invoke()\n",
    "    output_data = [interpreter.get_tensor(detail['index']) for detail in output_details]\n",
    "    return output_data\n",
    "\n",
    "def draw_bounding_boxes(image, output_data, input_size):\n",
    "    \"\"\"\n",
    "    Draws bounding boxes on the image based on the output data.\n",
    "    \n",
    "    Note: This function assumes a simplified output format. You may need to adjust it based on your model's actual output structure.\n",
    "    \"\"\"\n",
    "    # Assuming output_data contains bounding box coordinates and class probabilities\n",
    "    # Adjust indices and processing based on your model's output format\n",
    "    for detection in output_data[0][0]:  # Adjust index based on model output\n",
    "        scores = detection[5:]  # Class probabilities\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        \n",
    "        if confidence > 0.5:  # Confidence threshold\n",
    "            # Extract bounding box coordinates\n",
    "            x, y, w, h = detection[0:4] * np.array([input_size[1], input_size[0], input_size[1], input_size[0]])\n",
    "            x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "            \n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(image, (x - w // 2, y - h // 2), (x + w // 2, y + h // 2), (0, 255, 0), 2)\n",
    "            cv2.putText(image, f\"Class {class_id}\", (x - w // 2, y - h // 2 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Example usage:\n",
    "tflite_file = \"best-136_float16.tflite\"\n",
    "image_path = \"images/8.png\"  # Replace with your image path\n",
    "\n",
    "interpreter = load_tflite_model(tflite_file)\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "input_size = input_details[0]['shape'][1:3]  # Get input height and width\n",
    "\n",
    "preprocessed_image, original_image = preprocess_image(image_path, input_size)\n",
    "output_data = run_inference(interpreter, preprocessed_image)\n",
    "\n",
    "# Draw bounding boxes\n",
    "image_with_boxes = draw_bounding_boxes(original_image, output_data, input_size)\n",
    "\n",
    "# Display or save the image\n",
    "# cv2.imshow(\"Image with Bounding Boxes\", image_with_boxes)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# Save the image\n",
    "cv2.imwrite(\"output.jpg\", image_with_boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/silunikeerthiratne/Documents/IoT/inference/images/10.png: 768x1408 2 4s, 2 8s, 493.7ms\n",
      "Speed: 23.5ms preprocess, 493.7ms inference, 21.0ms postprocess per image at shape (1, 3, 768, 1408)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"best-136.pt\")  # pretrained YOLO11n model\n",
    "\n",
    "# Run batched inference on a list of images\n",
    "results = model(\"images/10.png\")  # return a list of Results objects\n",
    "\n",
    "# Process results list\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "    result.show()  # display to screen\n",
    "    result.save(filename=\"result.jpg\")  # save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([8., 4., 4., 8.])\n",
      "conf: tensor([0.8955, 0.8592, 0.8157, 0.8007])\n",
      "data: tensor([[6.1821e+02, 5.8083e+02, 1.0708e+03, 9.1033e+02, 8.9547e-01, 8.0000e+00],\n",
      "        [6.5660e+02, 2.4301e+02, 1.0107e+03, 5.8521e+02, 8.5920e-01, 4.0000e+00],\n",
      "        [1.7884e+03, 2.3505e+02, 2.1244e+03, 5.9381e+02, 8.1566e-01, 4.0000e+00],\n",
      "        [1.7539e+03, 6.0869e+02, 2.1892e+03, 9.2581e+02, 8.0068e-01, 8.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1500, 2800)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[ 844.4902,  745.5806,  452.5611,  329.5087],\n",
      "        [ 833.6587,  414.1104,  354.1232,  342.2008],\n",
      "        [1956.4309,  414.4302,  335.9688,  358.7605],\n",
      "        [1971.5364,  767.2517,  435.2762,  317.1246]])\n",
      "xywhn: tensor([[0.3016, 0.4971, 0.1616, 0.2197],\n",
      "        [0.2977, 0.2761, 0.1265, 0.2281],\n",
      "        [0.6987, 0.2763, 0.1200, 0.2392],\n",
      "        [0.7041, 0.5115, 0.1555, 0.2114]])\n",
      "xyxy: tensor([[ 618.2097,  580.8262, 1070.7708,  910.3350],\n",
      "        [ 656.5971,  243.0100, 1010.7203,  585.2108],\n",
      "        [1788.4465,  235.0499, 2124.4153,  593.8104],\n",
      "        [1753.8983,  608.6894, 2189.1746,  925.8140]])\n",
      "xyxyn: tensor([[0.2208, 0.3872, 0.3824, 0.6069],\n",
      "        [0.2345, 0.1620, 0.3610, 0.3901],\n",
      "        [0.6387, 0.1567, 0.7587, 0.3959],\n",
      "        [0.6264, 0.4058, 0.7818, 0.6172]])\n"
     ]
    }
   ],
   "source": [
    "print(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silunikeerthiratne/Documents/IoT/Code/myenvIot/lib/python3.11/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model\n",
    "model_path = \"best-136_float16.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = \"images/8.png\"\n",
    "image = cv2.imread(image_path)\n",
    "height, width, _ = image.shape\n",
    "\n",
    "# Resize the image to match the model's input size (e.g., 640x640)\n",
    "input_size = (1408, 1408)\n",
    "resized_image = cv2.resize(image, input_size)\n",
    "resized_image = resized_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Ensure input is FLOAT32\n",
    "input_image = np.expand_dims(resized_image, axis=0).astype(np.float32)\n",
    "\n",
    "# Set the input tensor to the preprocessed image\n",
    "interpreter.set_tensor(input_details[0]['index'], input_image)\n",
    "\n",
    "# Run the inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output tensor\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Assuming YOLOv9 output format (adjust based on actual output shape)\n",
    "# For simplicity, let's assume output is a single tensor with shape (1, num_boxes, 6)\n",
    "# where each box is represented as [x, y, w, h, confidence, class_id]\n",
    "\n",
    "# Interpret the output\n",
    "class_ids = []\n",
    "scores = []\n",
    "boxes = []\n",
    "\n",
    "# Assuming output_data is a numpy array\n",
    "for detection in output_data[0]:\n",
    "    scores.append(detection[4])  # Confidence\n",
    "    class_ids.append(int(detection[5]))  # Class ID\n",
    "    # Convert bounding box coordinates to original image scale\n",
    "    x, y, w, h = detection[0:4]\n",
    "    x = int((x - w / 2) * width)\n",
    "    y = int((y - h / 2) * height)\n",
    "    x2 = int((x + w) * width)\n",
    "    y2 = int((y + h) * height)\n",
    "    \n",
    "    # Ensure bounding box coordinates are within image bounds\n",
    "    x = max(0, min(x, width))\n",
    "    y = max(0, min(y, height))\n",
    "    x2 = max(0, min(x2, width))\n",
    "    y2 = max(0, min(y2, height))\n",
    "    \n",
    "    boxes.append([x, y, x2, y2])\n",
    "\n",
    "# Apply Non-Maximum Suppression (NMS) if needed\n",
    "# For simplicity, this step is omitted here. You can use OpenCV's NMSBoxes function.\n",
    "\n",
    "# Draw bounding boxes on the original image\n",
    "for i in range(len(boxes)):\n",
    "    if scores[i] > 0:  # Filter by confidence threshold\n",
    "        x, y, x2, y2 = boxes[i]\n",
    "        cv2.rectangle(image, (x, y), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f'Class {class_ids[i]} - {scores[i]:.2f}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image with bounding boxes\n",
    "# cv2.imshow('Image', image)\n",
    "cv2.imwrite(\"image2.jpeg\", image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[   0.009024    0.020137    0.021167 ...     0.92981     0.94922     0.96323]\n",
      "  [   0.018884    0.023064    0.020685 ...     0.96085     0.95678     0.96026]\n",
      "  [   0.023198    0.046142     0.04789 ...     0.13757     0.10063    0.074538]\n",
      "  ...\n",
      "  [ 7.3438e-06  1.4184e-05  1.2957e-05 ...  0.00017561  0.00019366  0.00015047]\n",
      "  [ 5.7905e-06  9.9166e-06  6.6689e-06 ...  0.00010213  0.00021677  0.00019057]\n",
      "  [ 8.7898e-06  1.5511e-05  1.3333e-05 ...  4.1668e-05  8.0066e-05  5.5776e-05]]]\n"
     ]
    }
   ],
   "source": [
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silunikeerthiratne/Documents/IoT/Code/myenvIot/lib/python3.11/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model\n",
    "model_path = \"best-169_float16.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = \"images/10.png\"\n",
    "image = cv2.imread(image_path)\n",
    "height, width, _ = image.shape\n",
    "\n",
    "# Resize the image to match the model's input size (e.g., 640x640)\n",
    "input_size = (1408, 1408)\n",
    "resized_image = cv2.resize(image, input_size)\n",
    "resized_image = resized_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Ensure input is FLOAT32\n",
    "input_image = np.expand_dims(resized_image, axis=0).astype(np.float32)\n",
    "\n",
    "# Set the input tensor to the preprocessed image\n",
    "interpreter.set_tensor(input_details[0]['index'], input_image)\n",
    "\n",
    "# Run the inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output tensor\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Assuming YOLOv9 output format (adjust based on actual output shape)\n",
    "# For simplicity, let's assume output is a tensor with shape (1, num_anchors, num_candidate_detections)\n",
    "# where each detection is represented as [x, y, w, h, confidence, class_probabilities]\n",
    "\n",
    "# Interpret the output\n",
    "class_ids = []\n",
    "scores = []\n",
    "boxes = []\n",
    "\n",
    "# Assuming output_data is a numpy array\n",
    "for anchor in output_data[0]:\n",
    "    for detection in anchor:\n",
    "        # Assuming detection format: [x, y, w, h, confidence, class_probabilities]\n",
    "        x, y, w, h, confidence, *class_probabilities = detection\n",
    "        \n",
    "        # Convert bounding box coordinates to original image scale\n",
    "        x = int(x * width)\n",
    "        y = int(y * height)\n",
    "        w = int(w * width)\n",
    "        h = int(h * height)\n",
    "        \n",
    "        # Calculate bounding box coordinates\n",
    "        x1 = max(0, int(x - w / 2))\n",
    "        y1 = max(0, int(y - h / 2))\n",
    "        x2 = min(width, int(x + w / 2))\n",
    "        y2 = min(height, int(y + h / 2))\n",
    "        \n",
    "        # Find class ID with highest probability\n",
    "        class_id = np.argmax(class_probabilities)\n",
    "        \n",
    "        # Append detection if confidence is above threshold\n",
    "        if confidence > 0.5:\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "            scores.append(confidence)\n",
    "            class_ids.append(class_id)\n",
    "\n",
    "# Draw bounding boxes on the original image\n",
    "for i in range(len(boxes)):\n",
    "    x1, y1, x2, y2 = boxes[i]\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    cv2.putText(image, f'Class {class_ids[i]} - {scores[i]:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image with bounding boxes\n",
    "cv2.imshow('Image', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1, 14, 40656)\n",
      "(40656,)\n"
     ]
    }
   ],
   "source": [
    "print(type(output_data))\n",
    "print(output_data.shape)\n",
    "# print(output_data[0])\n",
    "print(output_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Load the TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"best-136_float16.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensor details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load class labels (adjust based on your model)\n",
    "class_labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"] # Replace with your actual class names\n",
    "\n",
    "# Function to preprocess the image for inference\n",
    "def preprocess_image(image_path, input_shape):\n",
    "    \"\"\"\n",
    "    Preprocess the image to match the model's input requirements.\n",
    "    Args:\n",
    "        image_path: Path to the input image.\n",
    "        input_shape: Shape of the model's input tensor.\n",
    "    Returns:\n",
    "        Preprocessed image as a numpy array.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Ensure RGB format\n",
    "    image = image.resize((input_shape[1], input_shape[2]))  # Resize to model input size\n",
    "    image_array = np.array(image, dtype=np.float32) / 255.0  # Normalize pixel values\n",
    "    image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension\n",
    "    return image_array\n",
    "\n",
    "# Function to post-process inference results\n",
    "def postprocess_output(output_data, original_image_shape):\n",
    "    \"\"\"\n",
    "    Post-process the raw output data from the model.\n",
    "    Args:\n",
    "        output_data: Raw output data from the TensorFlow Lite model.\n",
    "        original_image_shape: Shape of the original input image (height, width).\n",
    "    Returns:\n",
    "        Bounding boxes in [xmin, ymin, xmax, ymax] format, class names, and confidence scores.\n",
    "    \"\"\"\n",
    "    boxes = []  # Bounding box coordinates\n",
    "    class_names = []  # Predicted class names\n",
    "    confidences = []  # Confidence scores\n",
    "    \n",
    "    # Example decoding logic (adjust based on your model's output format)\n",
    "    for detection in output_data[0]:  # Assuming output_data[0] contains bounding boxes\n",
    "        confidence = detection[4]  # Confidence score (adjust index based on your model)\n",
    "        if confidence > 0.5:  # Threshold for valid detections\n",
    "            x_min, y_min, x_max, y_max = detection[:4] * np.array(\n",
    "                [original_image_shape[1], original_image_shape[0], original_image_shape[1], original_image_shape[0]]\n",
    "            )\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            class_id = int(detection[5])  # Class ID (adjust index based on your model)\n",
    "            class_names.append(class_labels[class_id])  # Map ID to class name\n",
    "            confidences.append(confidence)\n",
    "    \n",
    "    return boxes, class_names, confidences\n",
    "\n",
    "# Function to draw bounding boxes on an image\n",
    "def draw_bounding_boxes(image, boxes, class_names=None, confidences=None, color=(0, 255, 0), thickness=2):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes on an image.\n",
    "    Args:\n",
    "        image: Input image (numpy array).\n",
    "        boxes: List of bounding boxes in [xmin, ymin, xmax, ymax] format.\n",
    "        class_names: List of predicted class names corresponding to each box.\n",
    "        confidences: List of confidence scores corresponding to each box.\n",
    "        color: Color of the bounding box (default is green).\n",
    "        thickness: Thickness of the box lines (default is 2).\n",
    "    Returns:\n",
    "        Annotated image with bounding boxes.\n",
    "    \"\"\"\n",
    "    for i, box in enumerate(boxes):\n",
    "        # Draw rectangle\n",
    "        cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color, thickness)\n",
    "        \n",
    "        # Add label and confidence if provided\n",
    "        if class_names and confidences:\n",
    "            label = f\"{class_names[i]} ({confidences[i]:.2f})\"\n",
    "            cv2.putText(image, label, (int(box[0]), int(box[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, thickness)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Path to your input image\n",
    "image_path = \"/Users/silunikeerthiratne/Documents/IoT/inference/images/10.png\"  # Replace with your actual image path\n",
    "\n",
    "# Load and preprocess the image for inference\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = preprocess_image(image_path, input_shape)\n",
    "\n",
    "# Perform inference\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get raw output data from the model\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Post-process output data to extract bounding boxes and labels\n",
    "original_image = cv2.imread(image_path)  # Load original image with OpenCV for drawing\n",
    "original_image_shape = original_image.shape[:2]  # Get height and width of original image\n",
    "\n",
    "bounding_boxes, predicted_classes, confidence_scores = postprocess_output(output_data, original_image_shape)\n",
    "\n",
    "# Draw bounding boxes on the original image\n",
    "annotated_image = draw_bounding_boxes(original_image.copy(), bounding_boxes, predicted_classes, confidence_scores)\n",
    "\n",
    "# Display and save the annotated image\n",
    "\n",
    "\n",
    "cv2.imwrite(\"output_image.jpg\", annotated_image)  # Save annotated image as a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(bounding_boxes)\n",
    "print(predicted_classes)\n",
    "print(confidence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40656,)\n"
     ]
    }
   ],
   "source": [
    "print(output_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 14, 40656)\n",
      "Image saved to output_image.jpg\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the TFLite model\n",
    "model_path = 'best-136_float16.tflite'\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Image preprocessing class\n",
    "class LetterBox:\n",
    "    \"\"\"Resize image and padding for detection.\"\"\"\n",
    "\n",
    "    def __init__(self, new_shape=(1408, 1408), auto=False, scaleFill=False, scaleup=True, stride=32):\n",
    "        self.new_shape = new_shape\n",
    "        self.auto = auto\n",
    "        self.scaleFill = scaleFill\n",
    "        self.scaleup = scaleup\n",
    "        self.stride = stride\n",
    "\n",
    "    def __call__(self, labels=None, image=None):\n",
    "        \"\"\"Return updated labels and image with added border.\"\"\"\n",
    "        if labels is None:\n",
    "            labels = {}\n",
    "        img = labels.get('img') if image is None else image\n",
    "        shape = img.shape[:2]  # current shape [height, width]\n",
    "        new_shape = labels.pop('rect_shape', self.new_shape)\n",
    "        if isinstance(new_shape, int):\n",
    "            new_shape = (new_shape, new_shape)\n",
    "\n",
    "        # Scale ratio (new / old)\n",
    "        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "        if not self.scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "            r = min(r, 1.0)\n",
    "\n",
    "        # Compute padding\n",
    "        ratio = r, r  # width, height ratios\n",
    "        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "        if self.auto:  # minimum rectangle\n",
    "            dw, dh = np.mod(dw, self.stride), np.mod(dh, self.stride)  # wh padding\n",
    "        elif self.scaleFill:  # stretch\n",
    "            dw, dh = 0.0, 0.0\n",
    "            new_unpad = (new_shape[1], new_shape[0])\n",
    "            ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "        dw /= 2  # divide padding into 2 sides\n",
    "        dh /= 2\n",
    "        if labels.get('ratio_pad'):\n",
    "            labels['ratio_pad'] = (labels['ratio_pad'], (dw, dh))  # for evaluation\n",
    "\n",
    "        if shape[::-1] != new_unpad:  # resize\n",
    "            img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "        top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "        left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
    "                                 value=(114, 114, 114))  # add border\n",
    "\n",
    "        if len(labels):\n",
    "            labels = self._update_labels(labels, ratio, dw, dh)\n",
    "            labels['img'] = img\n",
    "            labels['resized_shape'] = new_shape\n",
    "            return labels\n",
    "        else:\n",
    "            return img\n",
    "    \n",
    "    def _update_labels(self, labels, ratio, dw, dh):\n",
    "        # Placeholder for _update_labels function (Implement if needed)\n",
    "        return labels\n",
    "\n",
    "# Load image\n",
    "image_path = \"images/10.png\"\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Preprocess image\n",
    "letterbox = LetterBox(1408, auto=False, stride=32)\n",
    "im = letterbox(image=img)\n",
    "im = np.expand_dims(im, axis=0)\n",
    "im = im[..., ::-1].transpose((0, 1, 2, 3))  # BGR to RGB, BHWC to BCHW\n",
    "im = np.ascontiguousarray(im)\n",
    "im = im.astype(np.float32)\n",
    "im /= 255\n",
    "\n",
    "# Set input tensor\n",
    "input_data = im\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Process the output_data as needed\n",
    "print(output_data.shape)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "nc = 0\n",
    "conf_thres = 0.25\n",
    "\n",
    "bs = output_data.shape[0]  # batch size\n",
    "nc = nc or (output_data.shape[1] - 4)  # number of classes\n",
    "nm = output_data.shape[1] - nc - 4\n",
    "mi = 4 + nc  # mask start index\n",
    "xc = np.amax(output_data[:, 4:mi], 1) > conf_thres  # candidates\n",
    "\n",
    "multi_label=False\n",
    "multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
    "\n",
    "prediction = np.transpose(output_data, (0, 2, 1))\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    \"\"\"\n",
    "    Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the\n",
    "    top-left corner and (x2, y2) is the bottom-right corner.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x, y, width, height) format.\n",
    "    Returns:\n",
    "        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x1, y1, x2, y2) format.\n",
    "    \"\"\"\n",
    "    y = np.copy(x)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "prediction[..., :4] = xywh2xyxy(prediction[..., :4])  # xywh to xyxy\n",
    "\n",
    "output = [np.zeros((0, 6))] * bs\n",
    "\n",
    "import cv2\n",
    "\n",
    "max_nms=30000\n",
    "agnostic=False\n",
    "max_wh=7680\n",
    "iou_thres = 0.45\n",
    "max_det = 300\n",
    "\n",
    "nc = 10 # Number of classes\n",
    "\n",
    "for xi, x in enumerate(prediction):  # image index, image inference\n",
    "    # Filter based on confidence\n",
    "    conf = np.max(x[:, 4:4 + nc], axis=1)  # Confidence scores\n",
    "    confidence_mask = conf > conf_thres\n",
    "    x_filtered = x[confidence_mask]  # Apply confidence threshold\n",
    "\n",
    "    if x_filtered.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    # Apply NMS\n",
    "    boxes = x_filtered[:, :4]\n",
    "    scores = np.max(x_filtered[:, 4:4 + nc], axis=1)\n",
    "    class_ids = np.argmax(x_filtered[:, 4:4 + nc], axis=1)\n",
    "\n",
    "    nms_indices = cv2.dnn.NMSBoxes(boxes.astype(np.float32), scores, score_threshold=0.4, nms_threshold=iou_thres)\n",
    "\n",
    "    if nms_indices is not None and len(nms_indices) > 0:\n",
    "        nms_indices = nms_indices.flatten()\n",
    "        \n",
    "        # Extract the boxes that survive NMS\n",
    "        xyxy = boxes[nms_indices]\n",
    "        confidences = scores[nms_indices]\n",
    "        class_ids = class_ids[nms_indices]\n",
    "                \n",
    "        # Create output array\n",
    "        detections = np.concatenate([xyxy, confidences[:, None], class_ids[:, None]], axis=1) # Corrected axis\n",
    "        \n",
    "        output[xi] = detections  # Assign the results\n",
    "    else:\n",
    "        output[xi] = np.array([]) # No detections found\n",
    "        \n",
    "def clip_boxes(boxes, shape):\n",
    "    \"\"\"\n",
    "    It takes a list of bounding boxes and a shape (height, width) and clips the bounding boxes to the\n",
    "    shape... Args:\n",
    "      boxes (torch.Tensor): the bounding boxes to clip\n",
    "      shape (tuple): the shape of the image\n",
    "    \"\"\"\n",
    "    boxes[..., [0, 2]] = boxes[..., [0, 2]].clip(0, shape[1])  # x1, x2\n",
    "    boxes[..., [1, 3]] = boxes[..., [1, 3]].clip(0, shape[0])  # y1, y2\n",
    "\n",
    "def scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None):\n",
    "    \"\"\"\n",
    "    Rescales bounding boxes (in the format of xyxy) from the shape of the image they were originally specified in\n",
    "    (img1_shape) to the shape of a different image (img0_shape).\n",
    "\n",
    "    Args:\n",
    "      img1_shape (tuple): The shape of the image that the bounding boxes are for, in the format of (height, width).\n",
    "      boxes (torch.Tensor): the bounding boxes of the objects in the image, in the format of (x1, y1, x2, y2)\n",
    "      img0_shape (tuple): the shape of the target image, in the format of (height, width).\n",
    "      ratio_pad (tuple): a tuple of (ratio, pad) for scaling the boxes. If not provided, the ratio and pad will be\n",
    "                         calculated based on the size difference between the two images.\n",
    "\n",
    "    Returns:\n",
    "      boxes (torch.Tensor): The scaled bounding boxes, in the format of (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    if ratio_pad is None:  # calculate from img0_shape\n",
    "        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n",
    "        pad = round((img1_shape[1] - img0_shape[1] * gain) / 2 - 0.1), round(\n",
    "            (img1_shape[0] - img0_shape[0] * gain) / 2 - 0.1)  # wh padding\n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    boxes[..., [0, 2]] -= pad[0]  # x padding\n",
    "    boxes[..., [1, 3]] -= pad[1]  # y padding\n",
    "    boxes[..., :4] /= gain\n",
    "    clip_boxes(boxes, img0_shape)\n",
    "    return boxes\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, pred in enumerate(output):\n",
    "  if pred.size > 0:\n",
    "    pred[:, :4] = scale_boxes((1408, 1408), pred[:, :4], img.shape) # scale for each image\n",
    "  results.append(pred)\n",
    "\n",
    "# Drawing the bounding boxes and labels\n",
    "for detection in results:\n",
    "    if detection.size > 0:\n",
    "        for xmin, ymin, xmax, ymax, conf, class_id in detection:\n",
    "            # Convert coordinates to integers\n",
    "            xmin, ymin, xmax, ymax = map(int, [xmin, ymin, xmax, ymax])\n",
    "\n",
    "            # Draw rectangle and label\n",
    "            cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "            label = f\"Class {int(class_id)}: {conf:.2f}\"\n",
    "            cv2.putText(img, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "# Save the image to a directory\n",
    "output_path = 'output_image.jpg'\n",
    "cv2.imwrite(output_path, img)\n",
    "print(f\"Image saved to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[4.3926221e-01, 0.0000000e+00, 7.6024002e-01, 0.0000000e+00,\n",
      "        2.4129582e-08, 5.8059495e-06, 4.3060288e-07, 1.2242452e-06,\n",
      "        1.8564424e-04, 1.9738722e-05, 1.8518211e-04, 2.0621889e-04,\n",
      "        8.9767706e-01, 3.2575769e-04],\n",
      "       [4.6863478e-01, 0.0000000e+00, 7.1508098e-01, 0.0000000e+00,\n",
      "        5.6986668e-08, 5.7254629e-06, 7.0962611e-05, 8.2790095e-05,\n",
      "        8.5769618e-01, 6.4922460e-05, 3.1337936e-07, 1.8626173e-07,\n",
      "        2.0498404e-04, 6.1291698e-06],\n",
      "       [1.2708871e+00, 0.0000000e+00, 1.5083759e+00, 0.0000000e+00,\n",
      "        5.3224773e-07, 1.2056274e-05, 1.2673342e-04, 2.1024402e-04,\n",
      "        8.1394333e-01, 2.1741482e-04, 1.2471883e-06, 2.4431215e-06,\n",
      "        1.0089860e-03, 3.3942022e-05],\n",
      "       [1.2457570e+00, 0.0000000e+00, 1.5545688e+00, 0.0000000e+00,\n",
      "        3.9000167e-08, 3.6509023e-06, 3.4076655e-07, 3.7922376e-07,\n",
      "        1.8515845e-04, 6.2184913e-06, 4.2668878e-04, 4.6090828e-04,\n",
      "        8.0123442e-01, 1.4105260e-04]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Load the TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"/Users/silunikeerthiratne/Documents/IoT/inference/best-136_float16.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensor details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load class labels (adjust based on your model)\n",
    "class_labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]  # Replace with your actual class names\n",
    "# class_labels = [\"Class1\"]\n",
    "\n",
    "# Function to preprocess the image for inference\n",
    "def preprocess_image(image_path, input_shape):\n",
    "    \"\"\"\n",
    "    Preprocess the image to match the model's input requirements.\n",
    "    Args:\n",
    "        image_path: Path to the input image.\n",
    "        input_shape: Shape of the model's input tensor.\n",
    "    Returns:\n",
    "        Preprocessed image as a numpy array.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Ensure RGB format\n",
    "    image = image.resize((input_shape[1], input_shape[2]))  # Resize to model input size\n",
    "    image_array = np.array(image, dtype=np.float32) / 255.0  # Normalize pixel values\n",
    "    image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension\n",
    "    return image_array\n",
    "\n",
    "# Function to post-process inference results\n",
    "def postprocess_output(output_data, original_image_shape):\n",
    "    \"\"\"\n",
    "    Post-process the raw output data from the model.\n",
    "    Args:\n",
    "        output_data: Raw output data from the TensorFlow Lite model.\n",
    "        original_image_shape: Shape of the original input image (height, width).\n",
    "    Returns:\n",
    "        Bounding boxes in [xmin, ymin, xmax, ymax] format, class names, and confidence scores.\n",
    "    \"\"\"\n",
    "    boxes = []  # Bounding box coordinates\n",
    "    class_names = []  # Predicted class names\n",
    "    confidences = []  # Confidence scores\n",
    "\n",
    "    # Example decoding logic (adjust based on your model's output format)\n",
    "    for detection in output_data[0]:  # Assuming output_data[0] contains bounding boxes\n",
    "        confidence = detection[4]  # Confidence score (adjust index based on your model)\n",
    "        if confidence > 0.5:  # Threshold for valid detections\n",
    "            x_min, y_min, x_max, y_max = detection[:4] * np.array(\n",
    "                [original_image_shape[1], original_image_shape[0], original_image_shape[1], original_image_shape[0]]\n",
    "            )\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            class_id = int(detection[5])  # Class ID (adjust index based on your model)\n",
    "            class_names.append(class_labels[class_id])  # Map ID to class name\n",
    "            confidences.append(confidence)\n",
    "\n",
    "    return boxes, class_names, confidences\n",
    "\n",
    "# Function to draw bounding boxes on an image\n",
    "def draw_bounding_boxes(image, boxes, class_names=None, confidences=None, color=(0, 255, 0), thickness=2):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes on an image.\n",
    "    Args:\n",
    "        image: Input image (numpy array).\n",
    "        boxes: List of bounding boxes in [xmin, ymin, xmax, ymax] format.\n",
    "        class_names: List of predicted class names corresponding to each box.\n",
    "        confidences: List of confidence scores corresponding to each box.\n",
    "        color: Color of the bounding box (default is green).\n",
    "        thickness: Thickness of the box lines (default is 2).\n",
    "    Returns:\n",
    "        Annotated image with bounding boxes.\n",
    "    \"\"\"\n",
    "    for i, box in enumerate(boxes):\n",
    "        # Draw rectangle\n",
    "        cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color, thickness)\n",
    "\n",
    "        # Add label and confidence if provided\n",
    "        if class_names and confidences:\n",
    "            label = f\"{class_names[i]} ({confidences[i]:.2f})\"\n",
    "            cv2.putText(image, label, (int(box[0]), int(box[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, thickness)\n",
    "\n",
    "    return image\n",
    "\n",
    "# Path to your input image\n",
    "image_path = r\"/Users/silunikeerthiratne/Documents/IoT/inference/images/10.png\"  # Replace with your actual image path\n",
    "\n",
    "# Load and preprocess the image for inference\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = preprocess_image(image_path, input_shape)\n",
    "\n",
    "# Perform inference\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get raw output data from the model\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Post-process output data to extract bounding boxes and labels\n",
    "original_image = cv2.imread(image_path)  # Load original image with OpenCV for drawing\n",
    "original_image_shape = original_image.shape[:2]  # Get height and width of original image\n",
    "\n",
    "bounding_boxes, predicted_classes, confidence_scores = postprocess_output(output_data, original_image_shape)\n",
    "\n",
    "# Draw bounding boxes on the original image\n",
    "annotated_image = draw_bounding_boxes(original_image.copy(), bounding_boxes, predicted_classes, confidence_scores)\n",
    "\n",
    "# Display and save the annotated image\n",
    "\n",
    "\n",
    "cv2.imwrite(\"output_image.jpg\", annotated_image)  # Save annotated image as a file\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(bounding_boxes)\n",
    "print(predicted_classes)\n",
    "print(confidence_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'confidence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mconfidence\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'confidence' is not defined"
     ]
    }
   ],
   "source": [
    "print(confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silunikeerthiratne/Documents/IoT/Code/myenvIot/lib/python3.11/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path='best-136_float16.tflite')  # Update path\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "class LetterBox:\n",
    "    def __init__(self, new_shape=(1408, 1408), auto=False, scaleFill=False, scaleup=True, stride=32):\n",
    "        self.new_shape = new_shape\n",
    "        self.auto = auto\n",
    "        self.scaleFill = scaleFill\n",
    "        self.scaleup = scaleup\n",
    "        self.stride = stride\n",
    "\n",
    "    def __call__(self, labels=None, image=None):\n",
    "        if labels is None:\n",
    "            labels = {}\n",
    "        img = labels.get('img') if image is None else image\n",
    "        shape = img.shape[:2]\n",
    "        new_shape = labels.pop('rect_shape', self.new_shape)\n",
    "        if isinstance(new_shape, int):\n",
    "            new_shape = (new_shape, new_shape)\n",
    "\n",
    "        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "        if not self.scaleup:\n",
    "            r = min(r, 1.0)\n",
    "\n",
    "        ratio = r, r\n",
    "        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]\n",
    "        if self.auto:\n",
    "            dw, dh = np.mod(dw, self.stride), np.mod(dh, self.stride)\n",
    "        elif self.scaleFill:\n",
    "            dw, dh = 0.0, 0.0\n",
    "            new_unpad = (new_shape[1], new_shape[0])\n",
    "            ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]\n",
    "\n",
    "        dw /= 2\n",
    "        dh /= 2\n",
    "\n",
    "        if shape[::-1] != new_unpad:\n",
    "            img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "        top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "        left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
    "                                 value=(114, 114, 114))\n",
    "\n",
    "        if len(labels):\n",
    "            labels = self._update_labels(labels, ratio, dw, dh)\n",
    "            labels['img'] = img\n",
    "            labels['resized_shape'] = new_shape\n",
    "            return labels\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    y = np.copy(x)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2\n",
    "    return y\n",
    "\n",
    "def clip_boxes(boxes, shape):\n",
    "    boxes[..., [0, 2]] = boxes[..., [0, 2]].clip(0, shape[1])\n",
    "    boxes[..., [1, 3]] = boxes[..., [1, 3]].clip(0, shape[0])\n",
    "\n",
    "def scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None):\n",
    "    if ratio_pad is None:\n",
    "        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])\n",
    "        pad = round((img1_shape[1] - img0_shape[1] * gain) / 2 - 0.1), round(\n",
    "            (img1_shape[0] - img0_shape[0] * gain) / 2 - 0.1)\n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    boxes[..., [0, 2]] -= pad[0]\n",
    "    boxes[..., [1, 3]] -= pad[1]\n",
    "    boxes[..., :4] /= gain\n",
    "    clip_boxes(boxes, img0_shape)\n",
    "    return boxes\n",
    "\n",
    "def inference(image_path, conf_thres=0.25, iou_thres = 0.45, max_det = 300):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Image not found at the specified path.\")\n",
    "\n",
    "    # Preprocess the image\n",
    "    letterbox = LetterBox(1408, auto=False, stride=32)\n",
    "    im = letterbox(image=img.copy())\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    im = im[..., ::-1].transpose((0, 1, 2, 3))  # BGR to RGB, BHWC to BCHW\n",
    "    im = np.ascontiguousarray(im)  # contiguous\n",
    "    im = im.astype(np.float32)\n",
    "    im /= 255\n",
    "\n",
    "    # Set input tensor\n",
    "    input_data = im\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "    # Process output\n",
    "    prediction = np.transpose(output_data, (0, -1, -2))\n",
    "    prediction[..., :4] = xywh2xyxy(prediction[..., :4])\n",
    "\n",
    "    bs = prediction.shape[0]\n",
    "    nc = prediction.shape[2] - 4  # Number of classes\n",
    "    xc = np.amax(prediction[:, :, 4:4+nc], 2) > conf_thres  # candidates\n",
    "\n",
    "    output = [np.zeros((0, 6))] * bs  # Assuming no mask\n",
    "\n",
    "    max_nms=30000\n",
    "    agnostic=False\n",
    "    max_wh=7680\n",
    "\n",
    "    for xi, x in enumerate(prediction):\n",
    "        x = x[xc[xi]]\n",
    "\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        box = x[:, :4]\n",
    "        cls = x[:, 4:4+nc]\n",
    "\n",
    "        conf = np.max(cls, axis=1, keepdims=True)\n",
    "        j = np.argmax(cls, axis=1, keepdims=True)\n",
    "        x = np.concatenate((box, conf, j), axis=1)\n",
    "\n",
    "        conf_flat = conf.flatten()\n",
    "        filtered_x = x[conf_flat > conf_thres]\n",
    "\n",
    "        n = filtered_x.shape[0]\n",
    "        if not n:\n",
    "            continue\n",
    "\n",
    "        if n > max_nms:\n",
    "            sorted_indices = np.argsort(x[:, 4])[::-1]\n",
    "            x = x[sorted_indices[:max_nms]]\n",
    "\n",
    "        c = x[:, 5:6] * (0 if agnostic else max_wh)\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]\n",
    "\n",
    "        i = cv2.dnn.NMSBoxes(boxes, scores, score_threshold=conf_thres, nms_threshold=iou_thres)\n",
    "        if i is not None:\n",
    "            i = i[:max_det]\n",
    "            output[xi] = x[i.flatten()]\n",
    "        else:\n",
    "            output[xi] = np.array([])\n",
    "\n",
    "    # Scale boxes to original image size\n",
    "    for i, pred in enumerate(output):\n",
    "        if pred.size > 0:\n",
    "            pred[:, :4] = scale_boxes((1408, 1408), pred[:, :4], img.shape)\n",
    "            output[i] = pred\n",
    "\n",
    "    return img, output[0]\n",
    "\n",
    "def draw_boxes(img, results):\n",
    "    for *xyxy, conf, class_id in results:\n",
    "        xmin, ymin, xmax, ymax = map(int, xyxy)\n",
    "        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "        label = f\"Class {int(class_id)}: {conf:.2f}\"\n",
    "        cv2.putText(img, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    return img\n",
    "\n",
    "# Example usage:\n",
    "image_path = 'images/10.png'  # Specify your image path\n",
    "try:\n",
    "    img, results = inference(image_path)\n",
    "\n",
    "    # Draw bounding boxes on the image\n",
    "    img_with_boxes = draw_boxes(img.copy(), results)\n",
    "\n",
    "    # Display the image\n",
    "    cv2.imwrite('YOLOv8 Inference.jpg', img_with_boxes)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.43926221 0.         0.76024001 0.         0.89767706 8.        ]\n",
      " [0.46794514 0.         0.71700969 0.         0.86015636 4.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvIot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
