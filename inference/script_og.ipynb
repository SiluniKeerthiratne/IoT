{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silunikeerthiratne/Documents/IoT/Code/myenvIot/lib/python3.11/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Load the TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"best_50_v10_float16.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensor details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load class labels (adjust based on your model)\n",
    "class_labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]  # Replace with your actual class names\n",
    "\n",
    "# Function to preprocess the image for inference\n",
    "def preprocess_image(image_path, input_shape):\n",
    "    \"\"\"\n",
    "    Preprocess the image to match the model's input requirements.\n",
    "    Args:\n",
    "        image_path: Path to the input image.\n",
    "        input_shape: Shape of the model's input tensor.\n",
    "    Returns:\n",
    "        Preprocessed image as a numpy array.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Ensure RGB format\n",
    "    image = image.resize((input_shape[1], input_shape[2]))  # Resize to model input size\n",
    "    image_array = np.array(image, dtype=np.float32) / 255.0  # Normalize pixel values\n",
    "    image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension\n",
    "    return image_array\n",
    "\n",
    "# Function to post-process inference results\n",
    "def postprocess_output(output_data, original_image_shape):\n",
    "    \"\"\"\n",
    "    Post-process the raw output data from the model.\n",
    "    Args:\n",
    "        output_data: Raw output data from the TensorFlow Lite model.\n",
    "        original_image_shape: Shape of the original input image (height, width).\n",
    "    Returns:\n",
    "        Bounding boxes in [xmin, ymin, xmax, ymax] format, class names, and confidence scores.\n",
    "    \"\"\"\n",
    "    boxes = []  # Bounding box coordinates\n",
    "    class_names = []  # Predicted class names\n",
    "    confidences = []  # Confidence scores\n",
    "    \n",
    "    # Example decoding logic (adjust based on your model's output format)\n",
    "    for detection in output_data[0]:  # Assuming output_data[0] contains bounding boxes\n",
    "        confidence = detection[4]  # Confidence score (adjust index based on your model)\n",
    "        if confidence > 0.5:  # Threshold for valid detections\n",
    "            x_min, y_min, x_max, y_max = detection[:4] * np.array(\n",
    "                [original_image_shape[1], original_image_shape[0], original_image_shape[1], original_image_shape[0]]\n",
    "            )\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            class_id = int(detection[5])  # Class ID (adjust index based on your model)\n",
    "            class_names.append(class_labels[class_id])  # Map ID to class name\n",
    "            confidences.append(confidence)\n",
    "    \n",
    "    return boxes, class_names, confidences\n",
    "\n",
    "# Function to draw bounding boxes on an image\n",
    "def draw_bounding_boxes(image, boxes, class_names=None, confidences=None, color=(0, 255, 0), thickness=2):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes on an image.\n",
    "    Args:\n",
    "        image: Input image (numpy array).\n",
    "        boxes: List of bounding boxes in [xmin, ymin, xmax, ymax] format.\n",
    "        class_names: List of predicted class names corresponding to each box.\n",
    "        confidences: List of confidence scores corresponding to each box.\n",
    "        color: Color of the bounding box (default is green).\n",
    "        thickness: Thickness of the box lines (default is 2).\n",
    "    Returns:\n",
    "        Annotated image with bounding boxes.\n",
    "    \"\"\"\n",
    "    for i, box in enumerate(boxes):\n",
    "        # Draw rectangle\n",
    "        cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color, thickness)\n",
    "        \n",
    "        # Add label and confidence if provided\n",
    "        if class_names and confidences:\n",
    "            label = f\"{class_names[i]} ({confidences[i]:.2f})\"\n",
    "            cv2.putText(image, label, (int(box[0]), int(box[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, thickness)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Path to your input image\n",
    "image_path = \"images/10.png\"  # Replace with your actual image path\n",
    "\n",
    "# Load and preprocess the image for inference\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = preprocess_image(image_path, input_shape)\n",
    "\n",
    "# Perform inference\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get raw output data from the model\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Post-process output data to extract bounding boxes and labels\n",
    "original_image = cv2.imread(image_path)  # Load original image with OpenCV for drawing\n",
    "original_image_shape = original_image.shape[:2]  # Get height and width of original image\n",
    "\n",
    "bounding_boxes, predicted_classes, confidence_scores = postprocess_output(output_data, original_image_shape)\n",
    "\n",
    "# Draw bounding boxes on the original image\n",
    "annotated_image = draw_bounding_boxes(original_image.copy(), bounding_boxes, predicted_classes, confidence_scores)\n",
    "\n",
    "# Display and save the annotated image\n",
    "\n",
    "\n",
    "cv2.imwrite(\"output_image.jpg\", annotated_image)  # Save annotated image as a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1737.8199815750122, 617.6650822162628, 2205.705189704895, 899.7697234153748], [610.192745923996, 600.66819190979, 1080.33447265625, 887.7187371253967]]\n",
      "['8', '8']\n",
      "[0.68512034, 0.59686565]\n"
     ]
    }
   ],
   "source": [
    "print(bounding_boxes)\n",
    "print(predicted_classes)\n",
    "print(confidence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvIot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
